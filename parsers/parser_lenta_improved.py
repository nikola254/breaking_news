#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Å–∞–π—Ç–∞ Lenta.ru
–° –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
"""
import sys
import os
from datetime import datetime
import time
import random
import argparse

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from parsers.base_parser import BaseNewsParser
from bs4 import BeautifulSoup


class LentaParser(BaseNewsParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Lenta.ru"""
    
    def __init__(self, parse_period_hours: int = 24):
        super().__init__(
            source_name='lenta',
            base_url='https://lenta.ru',
            parse_period_hours=parse_period_hours,
            enable_duplicate_check=True,
            enable_classification=True,
            enable_preprocessing=True,
            min_confidence=0.15
        )
    
    def get_article_content(self, url: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ —Å Lenta.ru
        
        Args:
            url: URL —Å—Ç–∞—Ç—å–∏
            
        Returns:
            –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
        """
        html = self.fetch_url(url)
        if not html:
            return ""
        
        soup = self.parse_html(html)
        
        # –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å—Ç–∞—Ç—å–∏
        article_body = soup.find("div", class_="topic-body__content")
        
        if not article_body:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã
            article_body = soup.find("div", class_="b-text")
        
        if not article_body:
            article_body = soup.find("div", class_="js-topic__text")
        
        if not article_body:
            return "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = article_body.find_all(
            ["p", "div"],
            class_=lambda x: x != "topic-header"
        )
        
        content = "\n\n".join([
            p.get_text(strip=True)
            for p in paragraphs
            if p.get_text(strip=True)
        ])
        
        return content[:5000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä
    
    def parse_main_page(self):
        """–ü–∞—Ä—Å–∏—Ç –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É Lenta.ru"""
        print(f"\nüîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ Lenta.ru (–ø–µ—Ä–∏–æ–¥: {self.parse_period_hours} —á–∞—Å–æ–≤)...")
        
        html = self.fetch_url(self.base_url)
        if not html:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return
        
        soup = self.parse_html(html)
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç–∞—Ç—å–∏
        articles = soup.find_all("a", class_="card-full-news")
        
        if not articles:
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
            articles = soup.find_all("a", class_="card-mini")
        
        print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –Ω–∞ –≥–ª–∞–≤–Ω–æ–π: {len(articles)}")
        
        cutoff_date = self.get_cutoff_date()
        
        for article in articles:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                title_elem = article.find("span", class_="card-full-news__title")
                if not title_elem:
                    title_elem = article.find("h3")
                
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                link = article.get('href', '')
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if link and not link.startswith('http'):
                    link = f"{self.base_url}{link}"
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Å—ã–ª–∫–∏
                if not link or link == self.base_url:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏
                content = self.get_article_content(link)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä—É–±—Ä–∏–∫—É (–µ—Å–ª–∏ –µ—Å—Ç—å)
                rubric_elem = article.find("span", class_="card-full-news__rubric")
                rubric = rubric_elem.get_text(strip=True) if rubric_elem else ""
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é
                self.process_article(
                    title=title,
                    content=content,
                    link=link,
                    rubric=rubric,
                    published_date=datetime.now()
                )
                
                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
                self.stats['errors'] += 1
                continue
    
    def parse_rubric(self, rubric_url: str, rubric_name: str):
        """
        –ü–∞—Ä—Å–∏—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é —Ä—É–±—Ä–∏–∫—É
        
        Args:
            rubric_url: URL —Ä—É–±—Ä–∏–∫–∏
            rubric_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ä—É–±—Ä–∏–∫–∏
        """
        print(f"\nüìÇ –ü–∞—Ä—Å–∏–Ω–≥ —Ä—É–±—Ä–∏–∫–∏: {rubric_name}")
        
        html = self.fetch_url(rubric_url)
        if not html:
            return
        
        soup = self.parse_html(html)
        
        # –ù–∞—Ö–æ–¥–∏–º —Å—Ç–∞—Ç—å–∏ –≤ —Ä—É–±—Ä–∏–∫–µ
        articles = soup.find_all("li", class_="archive-page__item")
        
        print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –≤ —Ä—É–±—Ä–∏–∫–µ: {len(articles)}")
        
        for article in articles[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            try:
                link_elem = article.find("a")
                if not link_elem:
                    continue
                
                title = link_elem.get_text(strip=True)
                link = link_elem.get('href', '')
                
                if link and not link.startswith('http'):
                    link = f"{self.base_url}{link}"
                
                if not link:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                content = self.get_article_content(link)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º
                self.process_article(
                    title=title,
                    content=content,
                    link=link,
                    rubric=rubric_name,
                    published_date=datetime.now()
                )
                
                time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞: {e}")
                continue
    
    def parse(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        # –ü–∞—Ä—Å–∏–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
        self.parse_main_page()
        
        # –ü–∞—Ä—Å–∏–º –≤–∞–∂–Ω—ã–µ —Ä—É–±—Ä–∏–∫–∏
        important_rubrics = [
            ('https://lenta.ru/rubrics/world/', '–ú–∏—Ä'),
            ('https://lenta.ru/rubrics/russia/', '–†–æ—Å—Å–∏—è'),
            ('https://lenta.ru/rubrics/economics/', '–≠–∫–æ–Ω–æ–º–∏–∫–∞'),
        ]
        
        for rubric_url, rubric_name in important_rubrics:
            self.parse_rubric(rubric_url, rubric_name)
            time.sleep(random.uniform(1, 2))


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–ü–∞—Ä—Å–µ—Ä Lenta.ru —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π')
    parser.add_argument(
        '--hours',
        type=int,
        default=24,
        help='–ü–µ—Ä–∏–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ —á–∞—Å–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 24)'
    )
    parser.add_argument(
        '--no-duplicates-check',
        action='store_true',
        help='–û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤'
    )
    parser.add_argument(
        '--no-classification',
        action='store_true',
        help='–û—Ç–∫–ª—é—á–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("üöÄ LENTA.RU –ü–ê–†–°–ï–† (–£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)")
    print("=" * 60)
    print(f"‚è∞ –ü–µ—Ä–∏–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞: {args.hours} —á–∞—Å–æ–≤")
    print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {'–í–ö–õ' if not args.no_duplicates_check else '–í–´–ö–õ'}")
    print(f"ü§ñ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {'–í–ö–õ' if not args.no_classification else '–í–´–ö–õ'}")
    print("=" * 60)
    
    try:
        with LentaParser(parse_period_hours=args.hours) as lenta_parser:
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ–ø—Ü–∏–∏
            if args.no_duplicates_check:
                lenta_parser.enable_duplicate_check = False
            if args.no_classification:
                lenta_parser.enable_classification = False
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥
            lenta_parser.parse()
        
        print("\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

