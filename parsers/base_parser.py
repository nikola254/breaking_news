"""
–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≤—Å–µ—Ö –ø–∞—Ä—Å–µ—Ä–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
–í–∫–ª—é—á–∞–µ—Ç –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –∏ –ø—Ä–æ–≤–µ—Ä–∫—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
"""
import sys
import os
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
import requests
from bs4 import BeautifulSoup

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from clickhouse_driver import Client
from config import Config
from parsers.news_preprocessor import preprocessor
from parsers.gen_api_classifier import GenApiNewsClassifier
from parsers.duplicate_checker import create_duplicate_checker

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
try:
    from app.utils.ukraine_sentiment_analyzer import get_ukraine_sentiment_analyzer
    SENTIMENT_ANALYZER_AVAILABLE = True
except ImportError:
    SENTIMENT_ANALYZER_AVAILABLE = False
    print("Warning: UkraineSentimentAnalyzer not available")


def get_clickhouse_client():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞ ClickHouse"""
    return Client(
        host=Config.CLICKHOUSE_HOST,
        port=Config.CLICKHOUSE_NATIVE_PORT,
        user=Config.CLICKHOUSE_USER,
        password=Config.CLICKHOUSE_PASSWORD,
        database=Config.CLICKHOUSE_DATABASE
    )


class BaseNewsParser:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–µ—Ä–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(
        self,
        source_name: str,
        base_url: str,
        headers: Optional[Dict] = None,
        parse_period_hours: int = 24,
        enable_duplicate_check: bool = True,
        enable_classification: bool = True,
        enable_preprocessing: bool = True,
        min_confidence: float = 0.15
    ):
        """
        Args:
            source_name: –ù–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (lenta, rbc, gazeta –∏ —Ç.–¥.)
            base_url: –ë–∞–∑–æ–≤—ã–π URL —Å–∞–π—Ç–∞
            headers: HTTP –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è requests
            parse_period_hours: –ü–µ—Ä–∏–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ —á–∞—Å–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 24)
            enable_duplicate_check: –í–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            enable_classification: –í–∫–ª—é—á–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é
            enable_preprocessing: –í–∫–ª—é—á–∏—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞
            min_confidence: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        self.source_name = source_name
        self.base_url = base_url
        self.headers = headers or {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.parse_period_hours = parse_period_hours
        self.enable_duplicate_check = enable_duplicate_check
        self.enable_classification = enable_classification
        self.enable_preprocessing = enable_preprocessing
        self.min_confidence = min_confidence
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            'total_found': 0,
            'duplicates_skipped': 0,
            'low_confidence_skipped': 0,
            'successfully_saved': 0,
            'errors': 0,
            'by_category': {}
        }
        
        self.client = None
    
    def __enter__(self):
        """–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä - –æ—Ç–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"""
        self.client = get_clickhouse_client()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """–ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ"""
        if self.client:
            self.client.close()
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.print_stats()
    
    def fetch_url(self, url: str, timeout: int = 10) -> Optional[str]:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ URL
        
        Args:
            url: URL –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
            timeout: –¢–∞–π–º–∞—É—Ç –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            HTML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∏–ª–∏ None
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=timeout)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            return response.text
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            self.stats['errors'] += 1
            return None
    
    def parse_html(self, html: str) -> BeautifulSoup:
        """
        –ü–∞—Ä—Å–∏—Ç HTML —Å –ø–æ–º–æ—â—å—é BeautifulSoup
        
        Args:
            html: HTML —Å—Ç—Ä–æ–∫–∞
            
        Returns:
            BeautifulSoup –æ–±—ä–µ–∫—Ç
        """
        return BeautifulSoup(html, 'html.parser')
    
    def preprocess_article(self, title: str, content: str) -> Tuple[str, str]:
        """
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å—Ç–∞—Ç—å—é
        
        Args:
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ
            
        Returns:
            Tuple (cleaned_title, cleaned_content)
        """
        if not self.enable_preprocessing:
            return title, content
        
        return preprocessor.preprocess_article(title, content)
    
    def classify_article(self, title: str, content: str) -> Tuple[Optional[str], float, Dict]:
        """
        –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Å—Ç–∞—Ç—å—é
        
        Args:
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ
            
        Returns:
            Tuple (category, confidence, all_scores)
        """
        if not self.enable_classification:
            return None, 0.0, {}
        
        try:
            classifier = GenApiNewsClassifier()
            result = classifier.classify(title, content)
            
            return result['category_name'], result['confidence'], {
                'social_tension_index': result['social_tension_index'],
                'spike_index': result['spike_index']
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏: {e}")
            return None, 0.0, {}
    
    def check_duplicate(
        self,
        title: str,
        content: str,
        link: str,
        table_name: str
    ) -> Tuple[bool, str]:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—å—é –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç
        
        Args:
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ
            link: URL
            table_name: –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            Tuple (is_duplicate, reason)
        """
        if not self.enable_duplicate_check:
            return False, ""
        
        with create_duplicate_checker() as checker:
            return checker.is_duplicate(title, content, link, table_name)
    
    def save_article(
        self,
        title: str,
        content: str,
        link: str,
        category: str,
        table_name: str,
        published_date: Optional[datetime] = None,
        rubric: str = "",
        confidence: float = 1.0
    ) -> bool:
        """
        –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç—å—é –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö —Å –∞–Ω–∞–ª–∏–∑–æ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        
        Args:
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ
            link: URL
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è
            table_name: –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            published_date: –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            rubric: –†—É–±—Ä–∏–∫–∞
            confidence: –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            
        Returns:
            True –µ—Å–ª–∏ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ
        """
        if not self.client:
            print("–û—à–∏–±–∫–∞: —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –ë–î –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
            return False
        
        try:
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            from parsers.content_validator import ContentValidator
            
            validator = ContentValidator()
            source_domain = self._extract_domain(link)
            is_valid, cleaned_content = validator.validate_content(content, source_domain)
            
            if not is_valid:
                print(f"–°—Ç–∞—Ç—å—è –æ—Ç–∫–ª–æ–Ω–µ–Ω–∞ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–æ–º: {cleaned_content}")
                self.stats['validation_rejected'] = self.stats.get('validation_rejected', 0) + 1
                return False
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
            content = cleaned_content
            
            # AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ —Ä–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω–∞–ø—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç–∏
            ai_data = self._perform_ai_classification(title, content)
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞
            sentiment_data = {
                'sentiment_score': 0.0,
                'positive_score': 0.0,
                'negative_score': 0.0
            }
            
            if SENTIMENT_ANALYZER_AVAILABLE:
                try:
                    analyzer = get_ukraine_sentiment_analyzer()
                    text_for_analysis = f"{title} {content}"
                    sentiment_result = analyzer.analyze_sentiment(text_for_analysis)
                    
                    sentiment_data = {
                        'sentiment_score': sentiment_result.get('sentiment_score', 0.0),
                        'positive_score': sentiment_result.get('positive_score', 0.0),
                        'negative_score': sentiment_result.get('negative_score', 0.0)
                    }
                except Exception as e:
                    print(f"Warning: Sentiment analysis failed: {e}")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            insert_data = {
                'title': title,
                'link': link,
                'content': content,
                'rubric': rubric,
                'source': self.source_name,
                'category': category,
                'published_date': published_date or datetime.now(),
                'content_validated': 1,  # –§–ª–∞–≥ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                **sentiment_data,
                **ai_data
            }
            
            # SQL –∑–∞–ø—Ä–æ—Å —Å –ø–æ–ª—è–º–∏ sentiment, –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
            query = f"""
            INSERT INTO news.{table_name}
            (title, link, content, rubric, source, category, published_date, sentiment_score, positive_score, negative_score, content_validated, social_tension_index, spike_index, ai_classification_metadata, ai_category, ai_confidence)
            VALUES
            (%(title)s, %(link)s, %(content)s, %(rubric)s, %(source)s, %(category)s, %(published_date)s, %(sentiment_score)s, %(positive_score)s, %(negative_score)s, %(content_validated)s, %(social_tension_index)s, %(spike_index)s, %(ai_classification_metadata)s, %(ai_category)s, %(ai_confidence)s)
            """
            
            self.client.execute(query, insert_data)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self.stats['successfully_saved'] += 1
            self.stats['by_category'][category] = self.stats['by_category'].get(category, 0) + 1
            
            return True
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç—å–∏ '{title[:50]}...': {e}")
            self.stats['errors'] += 1
            return False
    
    def _extract_domain(self, url: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–æ–º–µ–Ω –∏–∑ URL
        
        Args:
            url: URL –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–æ–º–µ–Ω–∞
            
        Returns:
            str: –î–æ–º–µ–Ω
        """
        try:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            return parsed.netloc
        except Exception:
            return ""
    
    def _perform_ai_classification(self, title: str, content: str) -> dict:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é –Ω–æ–≤–æ—Å—Ç–∏ –∏ —Ä–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–æ–≤ –Ω–∞–ø—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç–∏
        
        Args:
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –Ω–æ–≤–æ—Å—Ç–∏
            
        Returns:
            dict: –î–∞–Ω–Ω—ã–µ AI-–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Gen-API –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä
            classifier = GenApiNewsClassifier()
            result = classifier.classify(title, content)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'gen_api_category': result['category_name'],
                'gen_api_category_id': result['category_id'],
                'gen_api_confidence': result['confidence'],
                'processing_time': 'gen_api',
                'cached': result.get('cached', False)
            }
            
            return {
                'social_tension_index': result['social_tension_index'],
                'spike_index': result['spike_index'],
                'ai_classification_metadata': str(metadata),
                'ai_category': result['category_name'],
                'ai_confidence': result['confidence']
            }
            
        except Exception as e:
            print(f"Warning: AI classification failed: {e}")
            
            # Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ä—É—á–Ω–æ–π —Ä–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–æ–≤
            try:
                from parsers.tension_calculator import calculate_both_indices
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                default_category = 'information_social'
                social_tension, spike_index = calculate_both_indices(
                    default_category, title, content
                )
                
                metadata = {
                    'fallback': True,
                    'error': str(e),
                    'default_category': default_category
                }
                
                return {
                    'social_tension_index': social_tension,
                    'spike_index': spike_index,
                    'ai_classification_metadata': str(metadata),
                    'ai_category': default_category,
                    'ai_confidence': 0.1  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è fallback
                }
                
            except Exception as e2:
                print(f"Warning: Fallback classification also failed: {e2}")
                
                # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback: –Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                return {
                    'social_tension_index': 0.0,
                    'spike_index': 0.0,
                    'ai_classification_metadata': f"{{'error': '{str(e2)}', 'fallback': True}}",
                    'ai_category': 'unknown',
                    'ai_confidence': 0.0
                }
    
    def process_article(
        self,
        title: str,
        content: str,
        link: str,
        rubric: str = "",
        published_date: Optional[datetime] = None
    ) -> bool:
        """
        –ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏:
        1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        2. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        3. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        
        Args:
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ
            link: URL
            rubric: –†—É–±—Ä–∏–∫–∞
            published_date: –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            
        Returns:
            True –µ—Å–ª–∏ —Å—Ç–∞—Ç—å—è —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞
        """
        self.stats['total_found'] += 1
        
        # 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        clean_title, clean_content = self.preprocess_article(title, content)
        
        # 2. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        category, confidence, scores = self.classify_article(clean_title, clean_content)
        
        if category is None or confidence < self.min_confidence:
            print(f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å {confidence:.2f}): {clean_title[:60]}...")
            self.stats['low_confidence_skipped'] += 1
            return False
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ç—å–∏ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π 'other' - –æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã –≤ –ë–î
        if category == 'other':
            print(f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–∫–∞—Ç–µ–≥–æ—Ä–∏—è 'other'): {clean_title[:60]}...")
            self.stats['low_confidence_skipped'] += 1
            return False
        
        # 3. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        category_table = f"{self.source_name}_{category}"
        
        # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ)
        is_dup, dup_reason = self.check_duplicate(
            clean_title,
            clean_content,
            link,
            category_table
        )
        
        if is_dup:
            print(f"‚ö†Ô∏è  –î—É–±–ª–∏–∫–∞—Ç: {clean_title[:60]}... ({dup_reason})")
            self.stats['duplicates_skipped'] += 1
            return False
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        headlines_table = f"{self.source_name}_headlines"
        is_dup_main, _ = self.check_duplicate(
            clean_title,
            clean_content,
            link,
            headlines_table
        )
        
        if is_dup_main:
            print(f"‚ö†Ô∏è  –î—É–±–ª–∏–∫–∞—Ç –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ: {clean_title[:60]}...")
            self.stats['duplicates_skipped'] += 1
            return False
        
        # 5. –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ–±–µ —Ç–∞–±–ª–∏—Ü—ã
        success_category = self.save_article(
            clean_title,
            clean_content,
            link,
            category,
            category_table,
            published_date,
            rubric,
            confidence
        )
        
        success_main = self.save_article(
            clean_title,
            clean_content,
            link,
            category,
            headlines_table,
            published_date,
            rubric,
            confidence
        )
        
        if success_category and success_main:
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ ({category}, {confidence:.2f}): {clean_title[:60]}...")
            return True
        
        return False
    
    def get_cutoff_date(self) -> datetime:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞—Ç—É –æ—Ç—Å–µ—á–∫–∏ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
        
        Returns:
            Datetime –æ–±—ä–µ–∫—Ç
        """
        return datetime.now() - timedelta(hours=self.parse_period_hours)
    
    def print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        print("\n" + "=" * 60)
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê: {self.source_name.upper()}")
        print("=" * 60)
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {self.stats['total_found']}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {self.stats['successfully_saved']}")
        print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats['duplicates_skipped']}")
        print(f"‚ùå –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {self.stats['low_confidence_skipped']}")
        print(f"üö´ –û—à–∏–±–æ–∫: {self.stats['errors']}")
        
        if self.stats['by_category']:
            print("\nüìë –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
            for cat, count in sorted(self.stats['by_category'].items()):
                print(f"  - {cat}: {count}")
        
        print("=" * 60 + "\n")
    
    def parse(self):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ - –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω –≤ –¥–æ—á–µ—Ä–Ω–∏—Ö –∫–ª–∞—Å—Å–∞—Ö
        """
        raise NotImplementedError("–ú–µ—Ç–æ–¥ parse() –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –≤ –¥–æ—á–µ—Ä–Ω–µ–º –∫–ª–∞—Å—Å–µ")

