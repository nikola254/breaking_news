#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä Gazeta.ru - —Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
"""
import sys
import os
import re
import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import random
import argparse

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from clickhouse_driver import Client
from config import Config
from parsers.news_preprocessor import preprocessor
from parsers.improved_classifier import classifier


class SimpleGazetaParser:
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä Gazeta.ru"""
    
    def __init__(self, parse_period_hours: int = 24):
        self.source_name = 'gazeta'
        self.base_url = 'https://www.gazeta.ru'
        self.parse_period_hours = parse_period_hours
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        self.stats = {
            'total_found': 0,
            'duplicates_skipped': 0,
            'low_confidence_skipped': 0,
            'successfully_saved': 0,
            'errors': 0,
            'by_category': {}
        }
        
        self.client = None
        self.seen_links = set()
    
    def connect_db(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î"""
        self.client = Client(
            host=Config.CLICKHOUSE_HOST,
            port=Config.CLICKHOUSE_NATIVE_PORT,
            user=Config.CLICKHOUSE_USER,
            password=Config.CLICKHOUSE_PASSWORD,
            database=Config.CLICKHOUSE_DATABASE
        )
    
    def close_db(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.client:
            try:
                self.client.disconnect()
            except:
                pass
    
    def get_article_content(self, url: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è Gazeta.ru
            body = None
            
            # –ù–æ–≤—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
            body = soup.find('div', class_='article_text_wrapper')
            
            if not body:
                body = soup.find('div', class_='b_article-text')
            
            if not body:
                body = soup.find('div', class_='article_text')
            
            if not body:
                body = soup.find('div', {'itemprop': 'articleBody'})
            
            if not body:
                # –ü—Ä–æ–±—É–µ–º article tag
                body = soup.find('article')
            
            if not body:
                return ""
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            paragraphs = body.find_all('p')
            if not paragraphs:
                # –ï—Å–ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –Ω–µ—Ç, –±–µ—Ä–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç
                content = body.get_text(strip=True)
            else:
                content = "\n\n".join([
                    p.get_text(strip=True)
                    for p in paragraphs
                    if p.get_text(strip=True) and len(p.get_text(strip=True)) > 10
                ])
            
            return content[:5000]
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
            return ""
    
    def is_duplicate_simple(self, link: str) -> bool:
        """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        if link in self.seen_links:
            return True
        
        if self.client:
            try:
                query = """
                SELECT count(*) as cnt
                FROM news.gazeta_headlines
                WHERE link = %(link)s
                """
                result = self.client.execute(query, {'link': link})
                if result and result[0][0] > 0:
                    return True
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–∞: {e}")
        
        return False
    
    def save_article(self, title: str, content: str, link: str, category: str, rubric: str = ""):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç—å—é –≤ –ë–î"""
        if not self.client:
            print("‚ùå –ù–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î")
            self.stats['errors'] += 1
            return False
        
        try:
            data = {
                'title': title,
                'link': link,
                'content': content,
                'rubric': rubric,
                'source': self.source_name,
                'category': category,
                'published_date': datetime.now()
            }
            
            # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
            query_main = """
            INSERT INTO news.gazeta_headlines
            (title, link, content, rubric, source, category, published_date)
            VALUES
            (%(title)s, %(link)s, %(content)s, %(rubric)s, %(source)s, %(category)s, %(published_date)s)
            """
            
            self.client.execute(query_main, data)
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–π–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
            category_table = f"gazeta_{category}"
            query_category = f"""
            INSERT INTO news.{category_table}
            (title, link, content, rubric, source, category, published_date)
            VALUES
            (%(title)s, %(link)s, %(content)s, %(rubric)s, %(source)s, %(category)s, %(published_date)s)
            """
            
            try:
                self.client.execute(query_category, data)
            except:
                pass
            
            self.stats['successfully_saved'] += 1
            self.stats['by_category'][category] = self.stats['by_category'].get(category, 0) + 1
            self.seen_links.add(link)
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            self.stats['errors'] += 1
            return False
    
    def process_article(self, title: str, link: str, rubric: str = ""):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É —Å—Ç–∞—Ç—å—é"""
        self.stats['total_found'] += 1
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–∞
        if self.is_duplicate_simple(link):
            print(f"‚ö†Ô∏è  –î—É–±–ª–∏–∫–∞—Ç: {title[:60]}...")
            self.stats['duplicates_skipped'] += 1
            return
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
        content = self.get_article_content(link)
        if not content:
            print(f"‚ùå –ù–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ: {title[:60]}...")
            self.stats['errors'] += 1
            return
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        clean_title, clean_content = preprocessor.preprocess_article(title, content)
        
        # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
        category, confidence, scores = classifier.classify(clean_title, clean_content)
        
        if category is None or confidence < 2.0:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ç—å—é
            print(f"‚ö†Ô∏è  –ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å ({confidence:.2f}): {clean_title[:60]}...")
            self.stats['low_confidence_skipped'] += 1
            return
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ç—å–∏ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π 'other' - –æ–Ω–∏ –Ω–µ –Ω—É–∂–Ω—ã –≤ –ë–î
        if category == 'other':
            print(f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–æ (–∫–∞—Ç–µ–≥–æ—Ä–∏—è 'other'): {clean_title[:60]}...")
            self.stats['low_confidence_skipped'] += 1
            return
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        success = self.save_article(clean_title, clean_content, link, category, rubric)
        
        if success:
            print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ [{category}, {confidence:.2f}]: {clean_title[:60]}...")
    
    def parse_main_page(self):
        """–ü–∞—Ä—Å–∏—Ç –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        print(f"\nüîç –ü–∞—Ä—Å–∏–Ω–≥ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã Gazeta.ru...")
        
        try:
            response = requests.get(self.base_url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
            articles = soup.find_all('a', href=lambda href: href and '/news/' in href)
            
            print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ —Å—Å—ã–ª–æ–∫ /news/: {len(articles)}")
            
            processed = 0
            for article in articles:
                if processed >= 50:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
                    break
                
                try:
                    link = article.get('href', '')
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                    if link and not link.startswith('http'):
                        link = f"{self.base_url}{link}"
                    
                    if not link or link == self.base_url or link.endswith('/news/'):
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    title = article.get_text(strip=True)
                    
                    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º—è –≤ –∫–æ–Ω—Ü–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (—Ñ–æ—Ä–º–∞—Ç: "—Ç–µ–∫—Å—Ç20:46")
                    title = re.sub(r'\d{2}:\d{2}$', '', title).strip()
                    
                    if not title or len(title) < 10:
                        continue
                    
                    self.process_article(title, link)
                    processed += 1
                    
                    # –ó–∞–¥–µ—Ä–∂–∫–∞
                    time.sleep(random.uniform(0.3, 0.8))
                    
                except Exception as e:
                    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                    continue
            
            print(f"\n‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed} —Å—Ç–∞—Ç–µ–π")
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≥–ª–∞–≤–Ω–æ–π: {e}")
            self.stats['errors'] += 1
    
    def parse_politics(self):
        """–ü–∞—Ä—Å–∏—Ç —Ä–∞–∑–¥–µ–ª –ø–æ–ª–∏—Ç–∏–∫–∏"""
        print(f"\nüîç –ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–∞ –ø–æ–ª–∏—Ç–∏–∫–∏...")
        
        try:
            url = f"{self.base_url}/politics"
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all('a', href=lambda href: href and '/politics/' in href and '/news/' in href)
            
            print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {len(articles)}")
            
            processed = 0
            for article in articles[:30]:
                try:
                    link = article.get('href', '')
                    if link and not link.startswith('http'):
                        link = f"{self.base_url}{link}"
                    
                    if not link or link.endswith('/politics/'):
                        continue
                    
                    title = article.get_text(strip=True)
                    
                    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º—è –≤ –∫–æ–Ω—Ü–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    title = re.sub(r'\d{2}:\d{2}$', '', title).strip()
                    
                    if not title or len(title) < 10:
                        continue
                    
                    self.process_article(title, link, rubric="–ü–æ–ª–∏—Ç–∏–∫–∞")
                    processed += 1
                    
                    time.sleep(random.uniform(0.3, 0.8))
                    
                except Exception as e:
                    continue
            
            print(f"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed} –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞—Ç–µ–π")
                    
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–æ–ª–∏—Ç–∏–∫–∏: {e}")
    
    def print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        print("\n" + "=" * 60)
        print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ê–†–°–ò–ù–ì–ê: {self.source_name.upper()}")
        print("=" * 60)
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {self.stats['total_found']}")
        print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {self.stats['successfully_saved']}")
        print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.stats['duplicates_skipped']}")
        print(f"‚ö†Ô∏è  –ù–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {self.stats['low_confidence_skipped']}")
        print(f"üö´ –û—à–∏–±–æ–∫: {self.stats['errors']}")
        
        if self.stats['by_category']:
            print("\nüìë –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
            for cat, count in sorted(self.stats['by_category'].items(), key=lambda x: x[1], reverse=True):
                print(f"  - {cat}: {count}")
        
        print("=" * 60 + "\n")
    
    def run(self):
        """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞"""
        try:
            self.connect_db()
            self.parse_main_page()
            self.parse_politics()
        finally:
            self.close_db()
            self.print_stats()


def main():
    parser = argparse.ArgumentParser(description='–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä Gazeta.ru')
    parser.add_argument('--hours', type=int, default=24, help='–ü–µ—Ä–∏–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ —á–∞—Å–∞—Ö')
    args = parser.parse_args()
    
    print("=" * 60)
    print("üöÄ GAZETA.RU –ü–ê–†–°–ï–† (–£–ü–†–û–©–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)")
    print("=" * 60)
    print(f"‚è∞ –ü–µ—Ä–∏–æ–¥: {args.hours} —á–∞—Å–æ–≤")
    print("=" * 60)
    
    try:
        gazeta_parser = SimpleGazetaParser(parse_period_hours=args.hours)
        gazeta_parser.run()
        print("\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω!")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

