#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–æ–≤–æ—Å—Ç–µ–π —Å —Å–∞–π—Ç–∞ Gazeta.ru
–° –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–æ–π, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–µ–π –∏ –ø—Ä–æ–≤–µ—Ä–∫–æ–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
"""
import sys
import os
from datetime import datetime
import time
import random
import argparse

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç–∏
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from parsers.base_parser import BaseNewsParser
from bs4 import BeautifulSoup


class GazetaParser(BaseNewsParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Gazeta.ru"""
    
    def __init__(self, parse_period_hours: int = 24):
        super().__init__(
            source_name='gazeta',
            base_url='https://www.gazeta.ru',
            parse_period_hours=parse_period_hours,
            enable_duplicate_check=True,
            enable_classification=True,
            enable_preprocessing=True,
            min_confidence=0.15
        )
    
    def get_article_content(self, url: str) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ —Å Gazeta.ru
        
        Args:
            url: URL —Å—Ç–∞—Ç—å–∏
            
        Returns:
            –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
        """
        html = self.fetch_url(url)
        if not html:
            return ""
        
        soup = self.parse_html(html)
        
        # –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
        article_body = soup.find("div", class_="b_article-text")
        
        if not article_body:
            article_body = soup.find("div", class_="article_text")
        
        if not article_body:
            article_body = soup.find("div", {"itemprop": "articleBody"})
        
        if not article_body:
            return "–°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å—Ç–∞—Ç—å–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
        paragraphs = article_body.find_all("p")
        
        content = "\n\n".join([
            p.get_text(strip=True)
            for p in paragraphs
            if p.get_text(strip=True) and len(p.get_text(strip=True)) > 10
        ])
        
        return content[:5000]
    
    def parse_main_page(self):
        """–ü–∞—Ä—Å–∏—Ç –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É Gazeta.ru"""
        print(f"\nüîç –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ Gazeta.ru (–ø–µ—Ä–∏–æ–¥: {self.parse_period_hours} —á–∞—Å–æ–≤)...")
        
        html = self.fetch_url(self.base_url)
        if not html:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É")
            return
        
        soup = self.parse_html(html)
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –Ω–æ–≤–æ—Å—Ç–∏
        articles = soup.find_all("a", class_="headline_main")
        
        if not articles:
            articles = soup.find_all("div", class_="b_ear-inner")
        
        print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –Ω–∞ –≥–ª–∞–≤–Ω–æ–π: {len(articles)}")
        
        for article in articles:
            try:
                if article.name == 'a':
                    link = article.get('href', '')
                    title = article.get_text(strip=True)
                else:
                    link_elem = article.find('a')
                    if not link_elem:
                        continue
                    link = link_elem.get('href', '')
                    title = link_elem.get_text(strip=True)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                if link and not link.startswith('http'):
                    link = f"{self.base_url}{link}"
                
                if not link or not title or link == self.base_url:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                content = self.get_article_content(link)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—å—é
                self.process_article(
                    title=title,
                    content=content,
                    link=link,
                    rubric="",
                    published_date=datetime.now()
                )
                
                time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç–∞—Ç—å–∏: {e}")
                self.stats['errors'] += 1
                continue
    
    def parse_politics(self):
        """–ü–∞—Ä—Å–∏—Ç —Ä–∞–∑–¥–µ–ª –ø–æ–ª–∏—Ç–∏–∫–∏"""
        print("\nüìÇ –ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–∞: –ü–æ–ª–∏—Ç–∏–∫–∞")
        
        url = f"{self.base_url}/politics"
        html = self.fetch_url(url)
        
        if not html:
            return
        
        soup = self.parse_html(html)
        articles = soup.find_all("div", class_="b_ear-inner")[:30]
        
        print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
        
        for article in articles:
            try:
                link_elem = article.find("a")
                if not link_elem:
                    continue
                
                title = link_elem.get_text(strip=True)
                link = link_elem.get('href', '')
                
                if link and not link.startswith('http'):
                    link = f"{self.base_url}{link}"
                
                if not link or not title:
                    continue
                
                content = self.get_article_content(link)
                
                self.process_article(
                    title=title,
                    content=content,
                    link=link,
                    rubric="–ü–æ–ª–∏—Ç–∏–∫–∞",
                    published_date=datetime.now()
                )
                
                time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞: {e}")
                continue
    
    def parse_news(self):
        """–ü–∞—Ä—Å–∏—Ç —Ä–∞–∑–¥–µ–ª –Ω–æ–≤–æ—Å—Ç–µ–π"""
        print("\nüìÇ –ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–∑–¥–µ–ª–∞: –ù–æ–≤–æ—Å—Ç–∏")
        
        url = f"{self.base_url}/news"
        html = self.fetch_url(url)
        
        if not html:
            return
        
        soup = self.parse_html(html)
        articles = soup.find_all("a", class_="headline")[:30]
        
        print(f"üì∞ –ù–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(articles)}")
        
        for article in articles:
            try:
                title = article.get_text(strip=True)
                link = article.get('href', '')
                
                if link and not link.startswith('http'):
                    link = f"{self.base_url}{link}"
                
                if not link or not title:
                    continue
                
                content = self.get_article_content(link)
                
                self.process_article(
                    title=title,
                    content=content,
                    link=link,
                    rubric="–ù–æ–≤–æ—Å—Ç–∏",
                    published_date=datetime.now()
                )
                
                time.sleep(random.uniform(0.5, 1.5))
                
            except Exception as e:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞: {e}")
                continue
    
    def parse(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        self.parse_main_page()
        self.parse_politics()
        self.parse_news()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–ü–∞—Ä—Å–µ—Ä Gazeta.ru —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π')
    parser.add_argument(
        '--hours',
        type=int,
        default=24,
        help='–ü–µ—Ä–∏–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤ —á–∞—Å–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 24)'
    )
    parser.add_argument(
        '--no-duplicates-check',
        action='store_true',
        help='–û—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –¥—É–±–ª–∏–∫–∞—Ç–æ–≤'
    )
    parser.add_argument(
        '--no-classification',
        action='store_true',
        help='–û—Ç–∫–ª—é—á–∏—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é'
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("üöÄ GAZETA.RU –ü–ê–†–°–ï–† (–£–õ–£–ß–®–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø)")
    print("=" * 60)
    print(f"‚è∞ –ü–µ—Ä–∏–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞: {args.hours} —á–∞—Å–æ–≤")
    print(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {'–í–ö–õ' if not args.no_duplicates_check else '–í–´–ö–õ'}")
    print(f"ü§ñ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: {'–í–ö–õ' if not args.no_classification else '–í–´–ö–õ'}")
    print("=" * 60)
    
    try:
        with GazetaParser(parse_period_hours=args.hours) as gazeta_parser:
            if args.no_duplicates_check:
                gazeta_parser.enable_duplicate_check = False
            if args.no_classification:
                gazeta_parser.enable_classification = False
            
            gazeta_parser.parse()
        
        print("\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()

